% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/igt_orl_regression.R
\name{igt_orl_regression}
\alias{igt_orl_regression}
\title{Outcome-Representation Learning Model}
\usage{
igt_orl_regression(
  data = NULL,
  exclude_cols = NULL,
  regression_pars = NULL,
  niter = 4000,
  nwarmup = 1000,
  nchain = 4,
  ncore = 1,
  nthin = 1,
  inits = "vb",
  indPars = "mean",
  modelRegressor = FALSE,
  vb = FALSE,
  inc_postpred = FALSE,
  adapt_delta = 0.95,
  stepsize = 1,
  max_treedepth = 10,
  ...
)
}
\arguments{
\item{data}{Data to be modeled. It should be given as a data.frame object,
a filepath for a tab-seperated txt file, \code{igt_example} to use example data, or
\code{"choose"} to choose data with an interactive window.
Columns in the dataset must include:
"subjID", "choice", "gain", "loss" and additional covariates. See \bold{Details} below for more information
on the representation of additional covariates.}

\item{exclude_cols}{A character vector of data columns to exclude from analysis.}

\item{regression_pars}{The parameters of the model that should be used for regression, supplied
as a character vector.}

\item{niter}{Number of iterations, including warm-up. Defaults to 4000.}

\item{nwarmup}{Number of iterations used for warm-up only. Defaults to 1000.}

\item{nchain}{Number of Markov chains to run. Defaults to 4.}

\item{ncore}{Number of CPUs to be used for running. Defaults to 1.}

\item{nthin}{Every \code{i == nthin} sample will be used to generate the posterior distribution.
Defaults to 1. A higher number can be used when auto-correlation within the MCMC sampling is
high.}

\item{inits}{Character value specifying how the initial values should be generated.
Possible options are "vb" (default), "fixed", "random", or your own initial values.}

\item{indPars}{Character value specifying how to summarize individual parameters. Current options
are: "mean", "median", or "mode".}

\item{modelRegressor}{Whether to export model-based regressors (\code{TRUE} or \code{FALSE}).
Not available for this model.}

\item{vb}{Use variational inference to approximately draw from a posterior distribution. Defaults
to \code{FALSE}.}

\item{inc_postpred}{Include trial-level posterior predictive simulations in model output (may greatly increase file
size). Defaults to \code{FALSE}.
If set to \code{TRUE}, it includes: "y_pred"}

\item{adapt_delta}{Floating point value representing the target acceptance probability of a new
sample in the MCMC chain. Must be between 0 and 1. See \bold{Details} below.}

\item{stepsize}{Integer value specifying the size of each leapfrog step that the MCMC sampler can
take on each new iteration. See \bold{Details} below.}

\item{max_treedepth}{Integer value specifying how many leapfrog steps the MCMC sampler can take
on each new iteration. See \bold{Details} below.}

\item{...}{For this model, it's possible to set \strong{model-specific argument(s)} as follows:
\describe{
\item{payscale}{Raw payoffs within data are divided by this number. Used for scaling data. Defaults to 100.}

}}
}
\value{
An object of class "hBayesDMregression" with the following components:
\describe{
\item{model}{Character value that is the name of the model (\\code{"igt_orl_regression"}).}
\item{modelPars}{Character vector indicating the parameters of the task model.}
\item{allIndPars}{Data.frame containing the summarized parameter values (as specified by
\code{indPars}) for each subject.}
\item{parVals}{List object containing the posterior samples over different parameters.}
\item{fit}{A class \code{\link[rstan]{stanfit}} object that contains the fitted Stan
model.}
\item{rawdata}{The raw data used to fit the model, as supplied by the user, returned as a
\code{data.frame} object.}

\item{modelRegressor}{List object containing the extracted model-based regressors.}
\item{model_code}{The Stan code used to fit the model. The Stan code can be printed nicely by
wrapping it with \code{cat()}.}
}
}
\description{
Hierarchical Bayesian Modeling of the Iowa Gambling Task using Outcome-Representation Learning Model.
It has the following parameters: \code{Arew} (reward learning rate), \code{Apun} (punishment learning rate), \code{K} (perseverance decay), \code{betaF} (outcome frequency weight), \code{betaP} (perseverance weight).

\itemize{
\item \strong{Task}: Iowa Gambling Task (Ahn et al., 2008)
\item \strong{Model}: Outcome-Representation Learning Model (Haines et al., 2018)
}
}
\details{
This section describes some of the function arguments in greater detail. For additional
mathematical details, see the Introduction vignette by calling
\code{vignette("introduction", package="hBayesDMregression")}.

\strong{data} should be assigned a character value specifying the full path and name (including
extension information, e.g. ".txt") of the file that contains the behavioral data-set of all
subjects of interest for the current analysis. The file should be a \strong{tab-delimited} text
file, whose rows represent trial-by-trial observations and columns represent variables.\cr
For the Iowa Gambling Task, there should be 4 columns of data with the
labels "subjID", "choice", "gain", "loss". It is not necessary for the columns to be in this particular order,
however it is necessary that they be labeled correctly and contain the information below:
\describe{
\item{subjID}{A unique identifier for each subject in the data-set.}
\item{choice}{Integer indicating which deck was chosen on that trial (where A==1, B==2, C==3, and D==4).}
\item{gain}{Floating point value representing the amount of currency won on that trial (e.g. 50, 100).}
\item{loss}{Floating point value representing the amount of currency lost on that trial (e.g. 0, -50).}

}

Categorical covariates should be one-hot encoded as 0s and 1s. See the \code{\link{igt_example}}
data set for an example of one-hot encoded covariates. A categorical covariate can be one-hot
encoded by following the code in
\href{https://gist.github.com/adamoshen/7cb505c485dc88508d23d03decb2fc84}{this example}

\strong{nwarmup} is a numerical value that specifies how many MCMC samples should not be stored
upon the beginning of each chain. For those familiar with Bayesian methods, this is equivalent
to burn-in samples. Due to the nature of the MCMC algorithm, initial values (i.e. where the
sampling chains begin) can have a heavy influence on the generated posterior distributions. The
\code{nwarmup} argument can be set to a high number in order to curb the effects that initial
values have on the resulting posteriors.

\strong{nchain} is a numerical value that specifies how many chains (i.e. independent sampling
sequences) should be used to draw samples from the posterior distribution. Since the posteriors
are generated from a sampling process, it is good practice to run multiple chains to ensure
that a reasonably representative posterior is attained. When the sampling is complete, it is
possible to check the multiple chains for convergence by running the following line of code:
\code{plot(output, type = "trace")}. The trace-plot should resemble a "furry caterpillar".

\strong{nthin} is a numerical value that specifies the "skipping" behavior of the MCMC sampler,
using only every \code{i == nthin} samples to generate posterior distributions. By default,
\code{nthin} is equal to 1, meaning that every sample is used to generate the posterior.

\strong{Control Parameters:} \code{adapt_delta}, \code{stepsize}, and \code{max_treedepth} are
advanced options that give the user more control over Stan's MCMC sampler. It is recommended
that only advanced users change the default values, as alterations can profoundly change the
sampler's behavior. Refer to 'The No-U-Turn Sampler: Adaptively Setting Path Lengths in
Hamiltonian Monte Carlo (Hoffman & Gelman, 2014, Journal of Machine Learning Research)' for
more information on the sampler control parameters. One can also refer to 'Section 34.2. HMC
Algorithm Parameters' of the \href{https://mc-stan.org/users/documentation/}{Stan User's Guide
and Reference Manual}, or to the help page for \code{\link[rstan]{stan}} for a less technical
description of these arguments.
}
\examples{
\dontrun{
# Run the model with a given data.frame as df
output <- igt_orl_regression(
  data = df, niter = 2000, nwarmup = 1000, nchain = 4, ncore = 4)

# Run the model with example data
output <- igt_orl_regression(
  data = "example", niter = 2000, nwarmup = 1000, nchain = 4, ncore = 4)

# Preview posterior samples of the regression coefficients; a matrix with dimension
# (niter - nwarmup) x (# of covariates)
head(output$parVals$beta)

# Preview posterior samples of the sigmas corresponding to the regression coefficients; same
# dimension as previous matrix
head(output$parVals$sigma_beta)

# View Stan code used to fit the model
cat(output$model_code)

# For visual diagnostics, see the plotting vignette:
# vignette(plotting, package="hBayesDMregression")
}
}
\references{
Ahn, W. Y., Busemeyer, J. R., & Wagenmakers, E. J. (2008). Comparison of decision learning models using the generalization criterion method. Cognitive Science, 32(8), 1376-1402. https://doi.org/10.1080/03640210802352992

Haines, N., Vassileva, J., & Ahn, W.-Y. (2018). The Outcome-Representation Learning Model: A Novel Reinforcement Learning Model of the Iowa Gambling Task. Cognitive Science. https://doi.org/10.1111/cogs.12688
}
